{"version":3,"file":"urql-exchange-graphcache.js","sources":["../src/ast/traversal.ts","../src/ast/schema.ts","../src/ast/schemaPredicates.ts","../src/helpers/dict.ts","../src/store/data.ts","../src/operations/shared.ts","../src/operations/write.ts","../src/ast/node.ts","../src/ast/variables.ts","../src/helpers/help.ts","../src/store/keys.ts","../src/store/store.ts","../src/operations/query.ts","../src/cacheExchange.ts","../src/offlineExchange.ts"],"sourcesContent":["import {\n  SelectionNode,\n  DocumentNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { invariant } from '../helpers/help';\nimport { Fragments, Variables } from '../types';\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  for (let i = 0; i < doc.definitions.length; i++) {\n    if (doc.definitions[i].kind === Kind.OPERATION_DEFINITION) {\n      return doc.definitions[i] as OperationDefinitionNode;\n    }\n  }\n\n  invariant(\n    false,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments => {\n  const fragments: Fragments = {};\n  for (let i = 0; i < doc.definitions.length; i++) {\n    const node = doc.definitions[i];\n    if (node.kind === Kind.FRAGMENT_DEFINITION) {\n      fragments[getName(node)] = node;\n    }\n  }\n\n  return fragments;\n};\n\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  const { directives } = node;\n  if (!directives) return true;\n\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0, l = directives.length; i < l; i++) {\n    const directive = directives[i];\n    const name = getName(directive);\n\n    if (\n      (name === 'include' || name === 'skip') &&\n      directive.arguments &&\n      directive.arguments[0] &&\n      getName(directive.arguments[0]) === 'if'\n    ) {\n      // Return whether this directive forces us to skip\n      // `@include(if: false)` or `@skip(if: true)`\n      const value = valueFromASTUntyped(directive.arguments[0].value, vars);\n      return name === 'include' ? !!value : !value;\n    }\n  }\n\n  return true;\n};\n","import {\n  IntrospectionQuery,\n  IntrospectionSchema,\n  IntrospectionInputValue,\n  IntrospectionTypeRef,\n  IntrospectionType,\n} from 'graphql';\n\nexport interface SchemaField {\n  name: string;\n  type: IntrospectionTypeRef;\n  args: Record<string, IntrospectionInputValue>;\n}\n\nexport interface SchemaObject {\n  name: string;\n  kind: 'INTERFACE' | 'OBJECT';\n  interfaces: Record<string, unknown>;\n  fields: Record<string, SchemaField>;\n}\n\nexport interface SchemaUnion {\n  name: string;\n  kind: 'UNION';\n  types: Record<string, unknown>;\n}\n\nexport interface SchemaIntrospector {\n  query: string | null;\n  mutation: string | null;\n  subscription: string | null;\n  types?: Record<string, SchemaObject | SchemaUnion>;\n  isSubType(abstract: string, possible: string): boolean;\n}\n\nexport interface PartialIntrospectionSchema {\n  queryType: { name: string; kind?: any };\n  mutationType?: { name: string; kind?: any };\n  subscriptionType?: { name: string; kind?: any };\n  types?: IntrospectionSchema['types'];\n}\n\nexport type IntrospectionData =\n  | IntrospectionQuery\n  | { __schema: PartialIntrospectionSchema };\n\nexport const buildClientSchema = ({\n  __schema,\n}: IntrospectionData): SchemaIntrospector => {\n  const typemap: Record<string, SchemaObject | SchemaUnion> = {};\n\n  const buildNameMap = <T extends { name: string }>(\n    arr: ReadonlyArray<T>\n  ): { [name: string]: T } => {\n    const map: Record<string, T> = {};\n    for (let i = 0; i < arr.length; i++) map[arr[i].name] = arr[i];\n    return map;\n  };\n\n  const buildType = (\n    type: IntrospectionType\n  ): SchemaObject | SchemaUnion | void => {\n    switch (type.kind) {\n      case 'OBJECT':\n      case 'INTERFACE':\n        return {\n          name: type.name,\n          kind: type.kind as 'OBJECT' | 'INTERFACE',\n          interfaces: buildNameMap(type.interfaces || []),\n          fields: buildNameMap(\n            type.fields.map(field => ({\n              name: field.name,\n              type: field.type,\n              args: buildNameMap(field.args),\n            }))\n          ),\n        } as SchemaObject;\n      case 'UNION':\n        return {\n          name: type.name,\n          kind: type.kind as 'UNION',\n          types: buildNameMap(type.possibleTypes || []),\n        } as SchemaUnion;\n    }\n  };\n\n  const schema: SchemaIntrospector = {\n    query: __schema.queryType ? __schema.queryType.name : null,\n    mutation: __schema.mutationType ? __schema.mutationType.name : null,\n    subscription: __schema.subscriptionType\n      ? __schema.subscriptionType.name\n      : null,\n    types: undefined,\n    isSubType(abstract: string, possible: string) {\n      const abstractType = typemap[abstract];\n      const possibleType = typemap[possible];\n      if (!abstractType || !possibleType) {\n        return false;\n      } else if (abstractType.kind === 'UNION') {\n        return !!abstractType.types[possible];\n      } else if (\n        abstractType.kind !== 'OBJECT' &&\n        possibleType.kind === 'OBJECT'\n      ) {\n        return !!possibleType.interfaces[abstract];\n      } else {\n        return abstract === possible;\n      }\n    },\n  };\n\n  if (__schema.types) {\n    schema.types = typemap;\n    for (let i = 0; i < __schema.types.length; i++) {\n      const type = __schema.types[i];\n      if (type && type.name) {\n        const out = buildType(type);\n        if (out) typemap[type.name] = out;\n      }\n    }\n  }\n\n  return schema;\n};\n","import { InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, invariant } from '../helpers/help';\nimport { getTypeCondition } from './node';\nimport { SchemaIntrospector, SchemaObject } from './schema';\n\nimport {\n  KeyingConfig,\n  UpdateResolver,\n  ResolverConfig,\n  OptimisticMutationConfig,\n} from '../types';\n\nconst BUILTIN_FIELD_RE = /^__/;\n\nexport const isFieldNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  if (BUILTIN_FIELD_RE.test(fieldName)) return true;\n  const field = getField(schema, typename, fieldName);\n  return !!field && field.type.kind !== 'NON_NULL';\n};\n\nexport const isListNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  if (!field) return false;\n  const ofType =\n    field.type.kind === 'NON_NULL' ? field.type.ofType : field.type;\n  return ofType.kind === 'LIST' && ofType.ofType.kind !== 'NON_NULL';\n};\n\nexport const isFieldAvailableOnType = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  if (BUILTIN_FIELD_RE.test(fieldName)) return true;\n  return !!getField(schema, typename, fieldName);\n};\n\nexport const isInterfaceOfType = (\n  schema: SchemaIntrospector,\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: string | void\n): boolean => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) return true;\n  if (\n    schema.types![typeCondition] &&\n    schema.types![typeCondition].kind === 'OBJECT'\n  )\n    return typeCondition === typename;\n  expectAbstractType(schema, typeCondition!);\n  expectObjectType(schema, typename!);\n  return schema.isSubType(typeCondition, typename);\n};\n\nconst getField = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n) => {\n  expectObjectType(schema, typename);\n  const object = schema.types![typename] as SchemaObject;\n  const field = object.fields[fieldName];\n  if (!field) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n  }\n\n  return field;\n};\n\nfunction expectObjectType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types![typename] && schema.types![typename].kind === 'OBJECT',\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types![typename] &&\n      (schema.types![typename].kind === 'INTERFACE' ||\n        schema.types![typename].kind === 'UNION'),\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n\nexport function expectValidKeyingConfig(\n  schema: SchemaIntrospector,\n  keys: KeyingConfig\n): void {\n  if (process.env.NODE_ENV !== 'production') {\n    for (const key in keys) {\n      if (!schema.types![key]) {\n        warn(\n          'Invalid Object type: The type `' +\n            key +\n            '` is not an object in the defined schema, but the `keys` option is referencing it.',\n          20\n        );\n      }\n    }\n  }\n}\n\nexport function expectValidUpdatesConfig(\n  schema: SchemaIntrospector,\n  updates: Record<string, Record<string, UpdateResolver>>\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const mutationFields = (schema.types![schema.mutation] as SchemaObject)\n      .fields;\n    const givenMutations = updates[schema.mutation] || {};\n    for (const fieldName in givenMutations) {\n      if (mutationFields[fieldName] === undefined) {\n        warn(\n          'Invalid mutation field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Mutation` option is referencing it.',\n          21\n        );\n      }\n    }\n  }\n\n  if (schema.subscription) {\n    const subscriptionFields = (schema.types![\n      schema.subscription\n    ] as SchemaObject).fields;\n    const givenSubscription = updates[schema.subscription] || {};\n    for (const fieldName in givenSubscription) {\n      if (subscriptionFields[fieldName] === undefined) {\n        warn(\n          'Invalid subscription field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Subscription` option is referencing it.',\n          22\n        );\n      }\n    }\n  }\n}\n\nfunction warnAboutResolver(name: string): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` is not in the defined schema, but the \\`resolvers\\` option is referencing it.`,\n    23\n  );\n}\n\nfunction warnAboutAbstractResolver(\n  name: string,\n  kind: 'UNION' | 'INTERFACE'\n): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` does not match to a concrete type in the schema, but the \\`resolvers\\` option is referencing it. Implement the resolver for the types that ${\n      kind === 'UNION' ? 'make up the union' : 'implement the interface'\n    } instead.`,\n    26\n  );\n}\n\nexport function expectValidResolversConfig(\n  schema: SchemaIntrospector,\n  resolvers: ResolverConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  for (const key in resolvers) {\n    if (key === 'Query') {\n      if (schema.query) {\n        const validQueries = (schema.types![schema.query] as SchemaObject)\n          .fields;\n        for (const resolverQuery in resolvers.Query) {\n          if (!validQueries[resolverQuery]) {\n            warnAboutResolver('Query.' + resolverQuery);\n          }\n        }\n      } else {\n        warnAboutResolver('Query');\n      }\n    } else {\n      if (!schema.types![key]) {\n        warnAboutResolver(key);\n      } else if (\n        schema.types![key].kind === 'INTERFACE' ||\n        schema.types![key].kind === 'UNION'\n      ) {\n        warnAboutAbstractResolver(\n          key,\n          schema.types![key].kind as 'INTERFACE' | 'UNION'\n        );\n      } else {\n        const validTypeProperties = (schema.types![key] as SchemaObject).fields;\n        for (const resolverProperty in resolvers[key]) {\n          if (!validTypeProperties[resolverProperty]) {\n            warnAboutResolver(key + '.' + resolverProperty);\n          }\n        }\n      }\n    }\n  }\n}\n\nexport function expectValidOptimisticMutationsConfig(\n  schema: SchemaIntrospector,\n  optimisticMutations: OptimisticMutationConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const validMutations = (schema.types![schema.mutation] as SchemaObject)\n      .fields;\n    for (const mutation in optimisticMutations) {\n      if (!validMutations[mutation]) {\n        warn(\n          `Invalid optimistic mutation field: \\`${mutation}\\` is not a mutation field in the defined schema, but the \\`optimistic\\` option is referencing it.`,\n          24\n        );\n      }\n    }\n  }\n}\n","export const makeDict = (): any => Object.create(null);\n\nexport const isDictEmpty = (x: any) => {\n  for (const _ in x) return false;\n  return true;\n};\n","import { stringifyVariables } from '@urql/core';\n\nimport {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n  Dependencies,\n  OperationType,\n} from '../types';\n\nimport {\n  serializeKeys,\n  deserializeKeyInfo,\n  fieldInfoOfKey,\n  joinKeys,\n} from './keys';\n\nimport { makeDict } from '../helpers/dict';\nimport { invariant, currentDebugStack } from '../helpers/help';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OptimisticMap<T> = Record<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OptimisticMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n}\n\nexport interface InMemoryData {\n  /** Flag for whether deferred tasks have been scheduled yet */\n  defer: boolean;\n  /** A list of entities that have been flagged for gargabe collection since no references to them are left */\n  gc: Set<string>;\n  /** A list of entity+field keys that will be persisted */\n  persist: Set<string>;\n  /** The API's \"Query\" typename which is needed to filter dependencies */\n  queryRootKey: string;\n  /** Number of references to each entity (except \"Query\") */\n  refCount: Dict<number>;\n  /** Number of references to each entity on optimistic layers */\n  refLock: OptimisticMap<Dict<number>>;\n  /** A map of entity fields (key-value entries per entity) */\n  records: NodeMap<EntityField>;\n  /** A map of entity links which are connections from one entity to another (key-value entries per entity) */\n  links: NodeMap<Link>;\n  /** A set of Query operation keys that are in-flight and awaiting a result */\n  commutativeKeys: Set<number>;\n  /** The order of optimistic layers */\n  optimisticOrder: number[];\n  /** This may be a persistence adapter that will receive changes in a batch */\n  storage: StorageAdapter | null;\n}\n\nlet currentOperation: null | OperationType = null;\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Dependencies = null;\nlet currentOptimisticKey: null | number = null;\nlet currentOptimistic = false;\n\nconst makeNodeMap = <T>(): NodeMap<T> => ({\n  optimistic: makeDict(),\n  base: new Map(),\n});\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  operationType: OperationType,\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  currentOperation = operationType;\n  currentData = data;\n  currentDependencies = makeDict();\n  currentOptimistic = !!isOptimistic;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  if (!layerKey) {\n    currentOptimisticKey = null;\n  } else if (isOptimistic || data.optimisticOrder.length > 0) {\n    // If this operation isn't optimistic and we see it for the first time,\n    // then it must've been optimistic in the past, so we can proactively\n    // clear the optimistic data before writing\n    if (!isOptimistic && !data.commutativeKeys.has(layerKey)) {\n      reserveLayer(data, layerKey);\n    } else if (isOptimistic) {\n      // NOTE: This optimally shouldn't happen as it implies that an optimistic\n      // write is being performed after a concrete write.\n      data.commutativeKeys.delete(layerKey);\n    }\n\n    // An optimistic update of a mutation may force an optimistic layer,\n    // or this Query update may be applied optimistically since it's part\n    // of a commutative chain\n    currentOptimisticKey = layerKey;\n    createLayer(data, layerKey);\n  } else {\n    // Otherwise we don't create an optimistic layer and clear the\n    // operation's one if it already exists\n    currentOptimisticKey = null;\n    deleteLayer(data, layerKey);\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  // NOTE: This is only called to check for the invariant to pass\n  if (process.env.NODE_ENV !== 'production') {\n    getCurrentDependencies();\n  }\n\n  const data = currentData!;\n  const layerKey = currentOptimisticKey;\n  currentOptimistic = false;\n  currentOptimisticKey = null;\n\n  // Determine whether the current operation has been a commutative layer\n  if (layerKey && data.optimisticOrder.indexOf(layerKey) > -1) {\n    // Squash all layers in reverse order (low priority upwards) that have\n    // been written already\n    let i = data.optimisticOrder.length;\n    while (\n      --i >= 0 &&\n      data.refLock[data.optimisticOrder[i]] &&\n      data.commutativeKeys.has(data.optimisticOrder[i])\n    ) {\n      squashLayer(data.optimisticOrder[i]);\n    }\n  }\n\n  currentOperation = null;\n  currentData = null;\n  currentDependencies = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  // Schedule deferred tasks if we haven't already\n  if (process.env.NODE_ENV !== 'test' && !data.defer) {\n    data.defer = true;\n    Promise.resolve().then(() => {\n      initDataState('read', data, null);\n      gc();\n      persistData();\n      clearDataState();\n      data.defer = false;\n    });\n  }\n};\n\n/** Initialises then resets the data state, which may squash this layer if necessary */\nexport const noopDataState = (\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  initDataState('read', data, layerKey, isOptimistic);\n  clearDataState();\n};\n\nexport const getCurrentOperation = (): OperationType => {\n  invariant(\n    currentOperation !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentOperation;\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Dependencies => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  defer: false,\n  gc: new Set(),\n  persist: new Set(),\n  queryRootKey,\n  refCount: makeDict(),\n  refLock: makeDict(),\n  links: makeNodeMap(),\n  records: makeNodeMap(),\n  commutativeKeys: new Set(),\n  optimisticOrder: [],\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  const keymap: KeyMap<Dict<T | undefined>> = currentOptimisticKey\n    ? map.optimistic[currentOptimisticKey]\n    : map.base;\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  let node: Dict<T | undefined> | undefined;\n  // A read may be initialised to skip layers until its own, which is useful for\n  // reading back written data. It won't skip over optimistic layers however\n  let skip =\n    !currentOptimistic &&\n    currentOperation === 'read' &&\n    currentOptimisticKey &&\n    currentData!.commutativeKeys.has(currentOptimisticKey);\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const layerKey = currentData!.optimisticOrder[i];\n    const optimistic = map.optimistic[layerKey];\n    // If we're reading starting from a specific layer, we skip until a match\n    skip = skip && layerKey !== currentOptimisticKey;\n    // If the node and node value exists it is returned, including undefined\n    if (\n      optimistic &&\n      (!skip || !currentData!.commutativeKeys.has(layerKey)) &&\n      (!currentOptimistic ||\n        currentOperation === 'write' ||\n        currentData!.commutativeKeys.has(layerKey)) &&\n      (node = optimistic.get(entityKey)) !== undefined &&\n      fieldKey in node\n    ) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gc */\nconst updateRCForEntity = (\n  gc: void | Set<string>,\n  refCount: Dict<number>,\n  entityKey: string,\n  by: number\n): void => {\n  // Retrieve the reference count\n  const count = refCount[entityKey] !== undefined ? refCount[entityKey] : 0;\n  // Adjust it by the \"by\" value\n  const newCount = (refCount[entityKey] = (count + by) | 0);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gc !== undefined) {\n    if (newCount <= 0) gc.add(entityKey);\n    else if (count <= 0 && newCount > 0) gc.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gc */\nconst updateRCForLink = (\n  gc: void | Set<string>,\n  refCount: Dict<number>,\n  link: Link | undefined,\n  by: number\n): void => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gc, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      const entityKey = link[i];\n      if (entityKey) {\n        updateRCForEntity(gc, refCount, entityKey, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n): void => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const optimistic = map.optimistic[currentData!.optimisticOrder[i]];\n    if (optimistic !== undefined) {\n      extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n    }\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = () => {\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  currentData!.gc.forEach((entityKey: string, _, batch: Set<string>) => {\n    // Check first whether the reference count is still 0\n    const rc = currentData!.refCount[entityKey] || 0;\n    if (rc > 0) {\n      batch.delete(entityKey);\n      return;\n    }\n\n    // Each optimistic layer may also still contain some references to marked entities\n    for (const layerKey in currentData!.refLock) {\n      const refCount = currentData!.refLock[layerKey];\n      const locks = refCount[entityKey] || 0;\n      // If the optimistic layer has any references to the entity, don't GC it,\n      // otherwise delete the reference count from the optimistic layer\n      if (locks > 0) return;\n      delete refCount[entityKey];\n    }\n\n    // Delete the reference count, and delete the entity from the GC batch\n    delete currentData!.refCount[entityKey];\n    batch.delete(entityKey);\n    currentData!.records.base.delete(entityKey);\n    const linkNode = currentData!.links.base.get(entityKey);\n    if (linkNode) {\n      currentData!.links.base.delete(entityKey);\n      for (const fieldKey in linkNode) {\n        updateRCForLink(batch, currentData!.refCount, linkNode[fieldKey], -1);\n      }\n    }\n  });\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies![entityKey] = true;\n    } else if (fieldKey !== undefined) {\n      currentDependencies![joinKeys(entityKey, fieldKey)] = true;\n    }\n  }\n};\n\nconst updatePersist = (entityKey: string, fieldKey: string) => {\n  if (!currentOptimistic && currentData!.storage) {\n    currentData!.persist.add(serializeKeys(entityKey, fieldKey));\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value?: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link?: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: Dict<number>;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gc: void | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount =\n      data.refLock[currentOptimisticKey] ||\n      (data.refLock[currentOptimisticKey] = makeDict());\n    links = data.links.optimistic[currentOptimisticKey];\n  } else {\n    refCount = data.refCount;\n    links = data.links.base;\n    gc = data.gc;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links && links.get(entityKey);\n  const prevLink = prevLinkNode && prevLinkNode[fieldKey];\n\n  // Update persistence batch and dependencies\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gc, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gc, refCount, link, 1);\n};\n\n/** Reserves an optimistic layer and preorders it */\nexport const reserveLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index === -1) {\n    // The new layer needs to be reserved in front of all other commutative\n    // keys but after all non-commutative keys (which are added by `forceUpdate`)\n    data.optimisticOrder.unshift(layerKey);\n  } else if (!data.commutativeKeys.has(layerKey)) {\n    // Protect optimistic layers from being turned into non-optimistic layers\n    // while preserving optimistic data\n    clearLayer(data, layerKey);\n    // If the layer was an optimistic layer prior to this call, it'll be converted\n    // to a new non-optimistic layer and shifted ahead\n    data.optimisticOrder.splice(index, 1);\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  data.commutativeKeys.add(layerKey);\n};\n\n/** Creates an optimistic layer of links and records */\nconst createLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.optimisticOrder.indexOf(layerKey) === -1) {\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  if (!data.refLock[layerKey]) {\n    data.refLock[layerKey] = makeDict();\n    data.links.optimistic[layerKey] = new Map();\n    data.records.optimistic[layerKey] = new Map();\n  }\n};\n\n/** Clears all links and records of an optimistic layer */\nconst clearLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.refLock[layerKey]) {\n    delete data.refLock[layerKey];\n    delete data.records.optimistic[layerKey];\n    delete data.links.optimistic[layerKey];\n  }\n};\n\n/** Deletes links and records of an optimistic layer, and the layer itself */\nconst deleteLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index > -1) {\n    data.optimisticOrder.splice(index, 1);\n    data.commutativeKeys.delete(layerKey);\n  }\n\n  clearLayer(data, layerKey);\n};\n\n/** Merges an optimistic layer of links and records into the base data */\nconst squashLayer = (layerKey: number) => {\n  // Hide current dependencies from squashing operations\n  const previousDependencies = currentDependencies;\n  currentDependencies = makeDict();\n\n  const links = currentData!.links.optimistic[layerKey];\n  if (links) {\n    links.forEach((keyMap, entityKey) => {\n      for (const fieldKey in keyMap)\n        writeLink(entityKey, fieldKey, keyMap[fieldKey]);\n    });\n  }\n\n  const records = currentData!.records.optimistic[layerKey];\n  if (records) {\n    records.forEach((keyMap, entityKey) => {\n      for (const fieldKey in keyMap)\n        writeRecord(entityKey, fieldKey, keyMap[fieldKey]);\n    });\n  }\n\n  currentDependencies = previousDependencies;\n  deleteLayer(currentData!, layerKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const persistData = () => {\n  if (currentData!.storage) {\n    currentOptimistic = true;\n    currentOperation = 'read';\n    const entries: SerializedEntries = makeDict();\n    currentData!.persist.forEach(key => {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      let x: void | Link | EntityField;\n      if ((x = readLink(entityKey, fieldKey)) !== undefined) {\n        entries[key] = `:${stringifyVariables(x)}`;\n      } else if ((x = readRecord(entityKey, fieldKey)) !== undefined) {\n        entries[key] = stringifyVariables(x);\n      } else {\n        entries[key] = undefined;\n      }\n    });\n\n    currentOptimistic = false;\n    currentData!.storage.writeData(entries);\n    currentData!.persist.clear();\n  }\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState('write', data, null);\n\n  for (const key in entries) {\n    const value = entries[key];\n    if (value !== undefined) {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      if (value[0] === ':') {\n        writeLink(entityKey, fieldKey, JSON.parse(value.slice(1)));\n      } else {\n        writeRecord(entityKey, fieldKey, JSON.parse(value));\n      }\n    }\n  }\n\n  clearDataState();\n  data.storage = storage;\n};\n","import { CombinedError } from '@urql/core';\nimport {\n  GraphQLError,\n  FieldNode,\n  InlineFragmentNode,\n  FragmentDefinitionNode,\n} from 'graphql';\n\nimport {\n  isInlineFragment,\n  getTypeCondition,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  isFieldNode,\n} from '../ast';\n\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\nimport { hasField } from '../store/data';\nimport { Store, keyOfField } from '../store';\nimport { Fragments, Variables, DataField, NullArray, Data } from '../types';\nimport { getFieldArguments, shouldInclude, isInterfaceOfType } from '../ast';\n\nexport interface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  parent: Data;\n  fieldName: string;\n  error: GraphQLError | undefined;\n  partial: boolean;\n  optimistic: boolean;\n  __internal: {\n    path: Array<string | number>;\n    errorMap: { [path: string]: GraphQLError } | undefined;\n  };\n}\n\nexport const contextRef: { current: Context | null } = { current: null };\n\n// Checks whether the current data field is a cache miss because of a GraphQLError\nexport const getFieldError = (ctx: Context): GraphQLError | undefined =>\n  ctx.__internal.path.length > 0 && ctx.__internal.errorMap\n    ? ctx.__internal.errorMap[ctx.__internal.path.join('.')]\n    : undefined;\n\nexport const makeContext = (\n  store: Store,\n  variables: Variables,\n  fragments: Fragments,\n  typename: string,\n  entityKey: string,\n  optimistic?: boolean,\n  error?: CombinedError | undefined\n): Context => {\n  const ctx: Context = {\n    store,\n    variables,\n    fragments,\n    parent: { __typename: typename },\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    error: undefined,\n    partial: false,\n    optimistic: !!optimistic,\n    __internal: {\n      path: [],\n      errorMap: undefined,\n    },\n  };\n\n  if (error && error.graphQLErrors) {\n    for (let i = 0; i < error.graphQLErrors.length; i++) {\n      const graphQLError = error.graphQLErrors[i];\n      if (graphQLError.path && graphQLError.path.length) {\n        if (!ctx.__internal.errorMap)\n          ctx.__internal.errorMap = Object.create(null);\n        ctx.__internal.errorMap![graphQLError.path.join('.')] = graphQLError;\n      }\n    }\n  }\n\n  return ctx;\n};\n\nexport const updateContext = (\n  ctx: Context,\n  data: Data,\n  typename: string,\n  entityKey: string,\n  fieldKey: string,\n  fieldName: string\n) => {\n  contextRef.current = ctx;\n  ctx.parent = data;\n  ctx.parentTypeName = typename;\n  ctx.parentKey = entityKey;\n  ctx.parentFieldKey = fieldKey;\n  ctx.fieldName = fieldName;\n  ctx.error = getFieldError(ctx);\n};\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  vars: Variables\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return !getSelectionSet(node).some(node => {\n    if (!isFieldNode(node)) return false;\n    const fieldKey = keyOfField(getName(node), getFieldArguments(node, vars));\n    return !hasField(entityKey, fieldKey);\n  });\n};\n\ninterface SelectionIterator {\n  (): FieldNode | undefined;\n}\n\nexport const makeSelectionIterator = (\n  typename: void | string,\n  entityKey: string,\n  select: SelectionSet,\n  ctx: Context\n): SelectionIterator => {\n  let childIterator: SelectionIterator | void;\n  let index = 0;\n\n  return function next() {\n    if (childIterator !== undefined) {\n      const node = childIterator();\n      if (node !== undefined) {\n        return node;\n      }\n\n      childIterator = undefined;\n      if (process.env.NODE_ENV !== 'production') {\n        popDebugNode();\n      }\n    }\n\n    while (index < select.length) {\n      const node = select[index++];\n      if (!shouldInclude(node, ctx.variables)) {\n        continue;\n      } else if (!isFieldNode(node)) {\n        // A fragment is either referred to by FragmentSpread or inline\n        const fragmentNode = !isInlineFragment(node)\n          ? ctx.fragments[getName(node)]\n          : node;\n\n        if (fragmentNode !== undefined) {\n          const isMatching = ctx.store.schema\n            ? isInterfaceOfType(ctx.store.schema, fragmentNode, typename)\n            : isFragmentHeuristicallyMatching(\n                fragmentNode,\n                typename,\n                entityKey,\n                ctx.variables\n              );\n\n          if (isMatching) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(typename, fragmentNode);\n            }\n\n            return (childIterator = makeSelectionIterator(\n              typename,\n              entityKey,\n              getSelectionSet(fragmentNode),\n              ctx\n            ))();\n          }\n        }\n      } else {\n        return node;\n      }\n    }\n  };\n};\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x === undefined ? null : (x as Data | NullArray<Data>);\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n  isFieldAvailableOnType,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n} from '../ast';\n\nimport { invariant, warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  NullArray,\n  Variables,\n  Data,\n  Link,\n  OperationRequest,\n  Dependencies,\n  EntityField,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n} from './shared';\n\nexport interface WriteResult {\n  data: null | Data;\n  dependencies: Dependencies;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  key?: number\n): WriteResult => {\n  initDataState('write', store.data, key || null);\n  const result = startWrite(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  key: number\n): WriteResult => {\n  if (process.env.NODE_ENV !== 'production') {\n    invariant(\n      getMainOperation(request.query).operation === 'mutation',\n      'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n        'This case is unsupported and should never occur.',\n      10\n    );\n  }\n\n  initDataState('write', store.data, key, true);\n  const result = startWrite(store, request, {} as Data, undefined, true);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  isOptimistic?: boolean\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { data, dependencies: getCurrentDependencies() };\n  const kind = store.rootFields[operation.operation];\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    kind,\n    kind,\n    !!isOptimistic,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(kind, operation);\n  }\n\n  writeSelection(ctx, kind, getSelectionSet(operation), data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Partial<Data>,\n  variables?: Variables\n) => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (!fragment) {\n    return warn(\n      'writeFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      11\n    );\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const dataToWrite = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(dataToWrite);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey,\n    undefined\n  );\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), dataToWrite);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: undefined | string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.rootFields['query'];\n  const isRoot = !isQuery && !!ctx.store.rootNames[entityKey!];\n  const typename = isRoot || isQuery ? entityKey : data.__typename;\n  if (!typename) {\n    warn(\n      \"Couldn't find __typename when writing.\\n\" +\n        \"If you're writing to the cache manually have to pass a `__typename` property on each entity in your data.\",\n      14\n    );\n    return;\n  } else if (!isRoot && !isQuery && entityKey) {\n    InMemoryData.writeRecord(entityKey, '__typename', typename);\n  }\n\n  const iterate = makeSelectionIterator(\n    typename,\n    entityKey || typename,\n    select,\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iterate())) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldAlias = getFieldAlias(node);\n    let fieldValue = data[fieldAlias];\n\n    if (process.env.NODE_ENV !== 'production') {\n      if (!isRoot && fieldValue === undefined) {\n        const advice = ctx.optimistic\n          ? '\\nYour optimistic result may be missing a field!'\n          : '';\n\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.' +\n            advice,\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.store.schema && typename && fieldName !== '__typename') {\n        isFieldAvailableOnType(ctx.store.schema, typename, fieldName);\n      }\n    }\n\n    // We simply skip all typenames fields and assume they've already been written above\n    if (fieldName === '__typename') continue;\n\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n\n    // Execute optimistic mutation functions on root fields\n    if (ctx.optimistic && isRoot) {\n      const resolver = ctx.store.optimisticMutations[fieldName];\n\n      if (!resolver) continue;\n      // We have to update the context to reflect up-to-date ResolveInfo\n      updateContext(ctx, data, typename, typename, fieldKey, fieldName);\n      fieldValue = data[fieldAlias] = ensureData(\n        resolver(fieldArgs || {}, ctx.store, ctx)\n      );\n    }\n\n    if (node.selectionSet) {\n      // Process the field and write links for the child entities that have been written\n      if (entityKey && !isRoot) {\n        const key = joinKeys(entityKey, fieldKey);\n        const link = writeField(\n          ctx,\n          getSelectionSet(node),\n          ensureData(fieldValue),\n          key\n        );\n        InMemoryData.writeLink(entityKey || typename, fieldKey, link);\n      } else {\n        writeField(ctx, getSelectionSet(node), ensureData(fieldValue));\n      }\n    } else if (entityKey && !isRoot) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(\n        entityKey || typename,\n        fieldKey,\n        (fieldValue !== null || !getFieldError(ctx)\n          ? fieldValue\n          : undefined) as EntityField\n      );\n    }\n\n    if (isRoot) {\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        updateContext(\n          ctx,\n          data,\n          typename,\n          typename,\n          joinKeys(typename, fieldKey),\n          fieldName\n        );\n\n        data[fieldName] = fieldValue;\n        updater(data, fieldArgs || {}, ctx.store, ctx);\n      }\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n};\n\n// A pattern to match typenames of types that are likely never keyable\nconst KEYLESS_TYPE_RE = /^__|PageInfo|(Connection|Edge)$/;\n\nconst writeField = (\n  ctx: Context,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>,\n  parentFieldKey?: string\n): Link | undefined => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      // Add the current index to the walked path before processing the link\n      ctx.__internal.path.push(i);\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = parentFieldKey\n        ? joinKeys(parentFieldKey, `${i}`)\n        : undefined;\n      // Recursively write array data\n      const links = writeField(ctx, select, data[i], indexKey);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return newData;\n  } else if (data === null) {\n    return getFieldError(ctx) ? undefined : null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const typename = data.__typename;\n\n  if (\n    parentFieldKey &&\n    !ctx.store.keys[data.__typename] &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !KEYLESS_TYPE_RE.test(typename)\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  const childKey = entityKey || parentFieldKey;\n  writeSelection(ctx, childKey, select, data);\n  return childKey || null;\n};\n","import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  Kind,\n} from 'graphql';\n\nexport type SelectionSet = ReadonlyArray<SelectionNode>;\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias ? node.alias.value : getName(node);\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet => (node.selectionSet ? node.selectionSet.selections : []);\n\nexport const getTypeCondition = (node: {\n  typeCondition?: NamedTypeNode;\n}): string | null => (node.typeCondition ? getName(node.typeCondition) : null);\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  const args = {};\n  let argsSize = 0;\n  if (node.arguments && node.arguments.length) {\n    for (let i = 0, l = node.arguments.length; i < l; i++) {\n      const arg = node.arguments[i];\n      const value = valueFromASTUntyped(arg.value, vars);\n      if (value !== undefined && value !== null) {\n        args[getName(arg)] = value;\n        argsSize++;\n      }\n    }\n  }\n\n  return argsSize > 0 ? args : null;\n};\n\n/** Returns a filtered form of variables with values missing that the query doesn't require */\nexport const filterVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n) => {\n  if (!input || !node.variableDefinitions) {\n    return undefined;\n  }\n\n  const vars = {};\n  for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n    const name = getName(node.variableDefinitions[i].variable);\n    vars[name] = input[name];\n  }\n\n  return vars;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n): Variables => {\n  const vars = {};\n  if (!input) return vars;\n\n  if (node.variableDefinitions) {\n    for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n      const def = node.variableDefinitions[i];\n      const name = getName(def.variable);\n      vars[name] =\n        input[name] === undefined && def.defaultValue\n          ? valueFromASTUntyped(def.defaultValue, input)\n          : input[name];\n    }\n  }\n\n  for (const key in input) {\n    if (!(key in vars)) vars[key] = input[key];\n  }\n\n  return vars;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/graphcache/errors.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\n\nexport type ErrorCode =\n  | 1\n  | 2\n  | 3\n  | 4\n  | 5\n  | 6\n  | 7\n  | 8\n  | 9\n  | 10\n  | 11\n  | 12\n  | 13\n  | 14\n  | 15\n  | 16\n  | 17\n  | 18\n  | 19\n  | 20\n  | 21\n  | 22\n  | 23\n  | 24\n  | 25\n  | 26;\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\n// URL unfurls to https://formidable.com/open-source/urql/docs/graphcache/errors/\nconst helpUrl = '\\nhttps://bit.ly/2XbVrpR#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const popDebugNode = () => currentDebugStack.pop();\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import { stringifyVariables } from '@urql/core';\nimport { Variables, FieldInfo, KeyInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: null | Variables) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const serializeKeys = (entityKey: string, fieldKey: string) =>\n  `${entityKey.replace(/\\./g, '%2e')}.${fieldKey}`;\n\nexport const deserializeKeyInfo = (key: string): KeyInfo => {\n  const dotIndex = key.indexOf('.');\n  const entityKey = key.slice(0, dotIndex).replace(/%2e/g, '.');\n  const fieldKey = key.slice(dotIndex + 1);\n  return { entityKey, fieldKey };\n};\n","import { DocumentNode } from 'graphql';\nimport { TypedDocumentNode, formatDocument, createRequest } from '@urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  Data,\n  QueryInput,\n  UpdatesConfig,\n  UpdateResolver,\n  OptimisticMutationConfig,\n  KeyingConfig,\n} from '../types';\n\nimport { invariant } from '../helpers/help';\nimport { contextRef } from '../operations/shared';\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidateEntity } from '../operations/invalidate';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\nimport {\n  IntrospectionData,\n  SchemaIntrospector,\n  buildClientSchema,\n  expectValidKeyingConfig,\n  expectValidUpdatesConfig,\n  expectValidResolversConfig,\n  expectValidOptimisticMutationsConfig,\n} from '../ast';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport interface StoreOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionData;\n}\n\nexport class Store implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: Record<string, Record<string, UpdateResolver>>;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schema?: SchemaIntrospector;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(opts?: StoreOpts) {\n    if (!opts) opts = {};\n\n    this.resolvers = opts.resolvers || {};\n    this.optimisticMutations = opts.optimistic || {};\n    this.keys = opts.keys || {};\n\n    let queryName = 'Query';\n    let mutationName = 'Mutation';\n    let subscriptionName = 'Subscription';\n    if (opts.schema) {\n      const schema = buildClientSchema(opts.schema);\n      queryName = schema.query || queryName;\n      mutationName = schema.mutation || mutationName;\n      subscriptionName = schema.subscription || subscriptionName;\n      // Only add schema introspector if it has types info\n      if (schema.types) this.schema = schema;\n    }\n\n    this.updates = {\n      [mutationName]: (opts.updates && opts.updates.Mutation) || {},\n      [subscriptionName]: (opts.updates && opts.updates.Subscription) || {},\n    };\n\n    this.rootFields = {\n      query: queryName,\n      mutation: mutationName,\n      subscription: subscriptionName,\n    };\n\n    this.rootNames = {\n      [queryName]: 'query',\n      [mutationName]: 'mutation',\n      [subscriptionName]: 'subscription',\n    };\n\n    this.data = InMemoryData.make(queryName);\n\n    if (this.schema && process.env.NODE_ENV !== 'production') {\n      expectValidKeyingConfig(this.schema, this.keys);\n      expectValidUpdatesConfig(this.schema, this.updates);\n      expectValidResolversConfig(this.schema, this.resolvers);\n      expectValidOptimisticMutationsConfig(\n        this.schema,\n        this.optimisticMutations\n      );\n    }\n  }\n\n  keyOfField = keyOfField;\n\n  keyOfEntity(data: Data | null | string) {\n    // In resolvers and updaters we may have a specific parent\n    // object available that can be used to skip to a specific parent\n    // key directly without looking at its incomplete properties\n    if (contextRef.current && data === contextRef.current.parent)\n      return contextRef.current!.parentKey;\n\n    if (data == null || typeof data === 'string') return data || null;\n    if (!data.__typename) return null;\n    if (this.rootNames[data.__typename]) return data.__typename;\n\n    let key: string | null | void;\n    if (this.keys[data.__typename]) {\n      key = this.keys[data.__typename](data);\n    } else if (data.id != null) {\n      key = `${data.id}`;\n    } else if (data._id != null) {\n      key = `${data._id}`;\n    }\n\n    return key ? `${data.__typename}:${key}` : null;\n  }\n\n  resolve(\n    entity: Data | string | null,\n    field: string,\n    args?: Variables\n  ): DataField {\n    const fieldKey = keyOfField(field, args);\n    const entityKey = this.keyOfEntity(entity);\n    if (!entityKey) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link || null;\n  }\n\n  resolveFieldByKey = this.resolve;\n\n  invalidate(\n    entity: Data | string | null,\n    field?: string,\n    args?: Variables | null\n  ) {\n    const entityKey = this.keyOfEntity(entity);\n\n    invariant(\n      entityKey,\n      \"Can't generate a key for invalidate(...).\\n\" +\n        'You have to pass an id or _id field or create a custom `keys` field for `' +\n        typeof entity ===\n        'object'\n        ? (entity as Data).__typename\n        : entity + '`.',\n      19\n    );\n\n    invalidateEntity(entityKey, field, args);\n  }\n\n  inspectFields(entity: Data | string | null): FieldInfo[] {\n    const entityKey = this.keyOfEntity(entity);\n    return entityKey ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery<T = Data, V = Variables>(\n    input: QueryInput<T, V>,\n    updater: (data: T | null) => T | null\n  ): void {\n    const request = createRequest<T, V>(input.query, input.variables as any);\n    request.query = formatDocument(request.query);\n    const output = updater(this.readQuery(request));\n    if (output !== null) {\n      startWrite(this, request, output as any);\n    }\n  }\n\n  readQuery<T = Data, V = Variables>(input: QueryInput<T, V>): T | null {\n    const request = createRequest(input.query, input.variables!);\n    request.query = formatDocument(request.query);\n    return read(this, request).data as T | null;\n  }\n\n  readFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    entity: string | Data | T,\n    variables?: V\n  ): T | null {\n    return readFragment(\n      this,\n      formatDocument(fragment),\n      entity,\n      variables as any\n    ) as T | null;\n  }\n\n  writeFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    data: T,\n    variables?: V\n  ): void {\n    writeFragment(this, formatDocument(fragment), data, variables as any);\n  }\n}\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  Variables,\n  Data,\n  DataField,\n  Link,\n  OperationRequest,\n  NullArray,\n  Dependencies,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentOperation,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n} from './shared';\n\nimport {\n  isFieldAvailableOnType,\n  isFieldNullable,\n  isListNullable,\n} from '../ast';\n\nexport interface QueryResult {\n  dependencies: Dependencies;\n  partial: boolean;\n  data: null | Data;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data,\n  error?: CombinedError | undefined,\n  key?: number\n): QueryResult => {\n  initDataState('read', store.data, (data && key) || null);\n  const result = read(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data,\n  error?: CombinedError | undefined\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.rootFields[operation.operation];\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    rootKey,\n    rootKey,\n    false,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  // NOTE: This may reuse \"previous result data\" as indicated by the\n  // `originalData` argument in readRoot(). This behaviour isn't used\n  // for readSelection() however, which always produces results from\n  // scratch\n  const data =\n    rootKey !== ctx.store.rootFields['query']\n      ? readRoot(ctx, rootKey, rootSelect, input || ({} as Data))\n      : readSelection(ctx, rootKey, rootSelect, {} as Data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: ctx.partial || !data,\n    data: data || null,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  originalData: Data\n): Data => {\n  const typename = ctx.store.rootNames[entityKey]\n    ? entityKey\n    : originalData.__typename;\n  if (typeof typename !== 'string') {\n    return originalData;\n  }\n\n  const iterate = makeSelectionIterator(entityKey, entityKey, select, ctx);\n  const data = { __typename: typename };\n\n  let node: FieldNode | void;\n  while ((node = iterate())) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = originalData[fieldAlias];\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n    // Process the root field's value\n    if (node.selectionSet && fieldValue !== null) {\n      const fieldData = ensureData(fieldValue);\n      data[fieldAlias] = readRootField(ctx, getSelectionSet(node), fieldData);\n    } else {\n      data[fieldAlias] = fieldValue;\n    }\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n\n  return data;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: null | Data | NullArray<Data>\n): Data | NullArray<Data> | null => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    for (let i = 0, l = originalData.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the root field's value\n      newData[i] = readRootField(ctx, select, originalData[i]);\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return newData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    const fieldValue = readSelection(ctx, entityKey, select, {} as Data);\n    return fieldValue === undefined ? null : fieldValue;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Partial<Data> | string,\n  variables?: Variables\n): Data | null => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (!fragment) {\n    warn(\n      'readFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      6\n    );\n\n    return null;\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename)\n    entity.__typename = typename;\n  const entityKey = store.keyOfEntity(entity as Data);\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey\n  );\n\n  const result =\n    readSelection(ctx, entityKey, getSelectionSet(fragment), {} as Data) ||\n    null;\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nconst readSelection = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  data: Data,\n  result?: Data\n): Data | undefined => {\n  const { store } = ctx;\n  const isQuery = key === store.rootFields['query'];\n\n  const entityKey = (result && store.keyOfEntity(result)) || key;\n  if (!isQuery && !!ctx.store.rootNames[entityKey]) {\n    warn(\n      'Invalid root traversal: A selection was being read on `' +\n        entityKey +\n        '` which is an uncached root type.\\n' +\n        'The `' +\n        ctx.store.rootFields.mutation +\n        '` and `' +\n        ctx.store.rootFields.subscription +\n        '` types are special ' +\n        'Operation Root Types and cannot be read back from the cache.',\n      25\n    );\n  }\n\n  const typename = !isQuery\n    ? InMemoryData.readRecord(entityKey, '__typename') ||\n      (result && result.__typename)\n    : key;\n\n  if (typeof typename !== 'string') {\n    return;\n  } else if (result && typename !== result.__typename) {\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return;\n  }\n\n  const iterate = makeSelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iterate()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result ? result[fieldName] : undefined;\n    const resolvers = store.resolvers[typename];\n\n    if (process.env.NODE_ENV !== 'production' && store.schema && typename) {\n      isFieldAvailableOnType(store.schema, typename, fieldName);\n    }\n\n    // We directly assign typenames and skip the field afterwards\n    if (fieldName === '__typename') {\n      data[fieldAlias] = typename;\n      continue;\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n\n    if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (\n      getCurrentOperation() === 'read' &&\n      resolvers &&\n      typeof resolvers[fieldName] === 'function'\n    ) {\n      // We have to update the information in context to reflect the info\n      // that the resolver will receive\n      updateContext(ctx, data, typename, entityKey, key, fieldName);\n\n      // We have a resolver for this field.\n      // Prepare the actual fieldValue, so that the resolver can use it\n      if (fieldValue !== undefined) {\n        data[fieldAlias] = fieldValue;\n      }\n\n      dataFieldValue = resolvers[fieldName](\n        data,\n        fieldArgs || ({} as Data),\n        store,\n        ctx\n      );\n\n      if (node.selectionSet) {\n        // When it has a selection set we are resolving an entity with a\n        // subselection. This can either be a list or an object.\n        dataFieldValue = resolveResolverResult(\n          ctx,\n          typename,\n          fieldName,\n          key,\n          getSelectionSet(node),\n          data[fieldAlias] as Data,\n          dataFieldValue\n        );\n      }\n\n      if (\n        store.schema &&\n        dataFieldValue === null &&\n        !isFieldNullable(store.schema, typename, fieldName)\n      ) {\n        // Special case for when null is not a valid value for the\n        // current field\n        return undefined;\n      }\n    } else if (!node.selectionSet) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        data[fieldAlias] as Data,\n        resultValue\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // If we have an error registered for the current field change undefined values to null\n    if (dataFieldValue === undefined && !!getFieldError(ctx)) {\n      hasPartials = true;\n      dataFieldValue = null;\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      store.schema &&\n      isFieldNullable(store.schema, typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return isQuery && hasPartials && !hasFields ? undefined : data;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  result: void | DataField\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { store } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const _isListNullable =\n      !store.schema || isListNullable(store.schema, typename, fieldName);\n    const data = new Array(result.length);\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        // Get the inner previous data from prevData\n        prevData != null ? prevData[i] : undefined,\n        result[i]\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childResult === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        data[i] = childResult !== undefined ? childResult : null;\n      }\n    }\n\n    return data;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (prevData === null) {\n    // If we've previously set this piece of data to be null,\n    // we skip it and return null immediately\n    return null;\n  } else if (isDataOrKey(result)) {\n    const data = (prevData || {}) as Data;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readSelection(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[]\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { store } = ctx;\n    const _isListNullable =\n      store.schema && isListNullable(store.schema, typename, fieldName);\n    const newLink = new Array(link.length);\n    for (let i = 0, l = link.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the link\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData != null ? prevData[i] : undefined\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childLink === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        newLink[i] = childLink !== undefined ? childLink : null;\n      }\n    }\n\n    return newLink;\n  } else if (link === null || prevData === null) {\n    // If the link is set to null or we previously set this piece of data to be null,\n    // we skip it and return null immediately\n    return null;\n  } else {\n    return readSelection(ctx, link, select, (prevData || {}) as Data);\n  }\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import {\n  Exchange,\n  formatDocument,\n  makeOperation,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from '@urql/core';\n\nimport {\n  filter,\n  combine,\n  scan,\n  map,\n  merge,\n  pipe,\n  share,\n  fromPromise,\n  fromArray,\n  take,\n  mergeMap,\n  concat,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { makeDict, isDictEmpty } from './helpers/dict';\nimport { addCacheOutcome, toRequestPolicy } from './helpers/operation';\nimport { IntrospectionData, filterVariables, getMainOperation } from './ast';\nimport { Store, noopDataState, hydrateData, reserveLayer } from './store';\n\nimport {\n  UpdatesConfig,\n  ResolverConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  StorageAdapter,\n  Dependencies,\n} from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n  dependencies: Dependencies;\n};\n\ntype Operations = Set<number>;\ntype OperationMap = Map<number, Operation>;\ntype OptimisticDependencies = Map<number, Dependencies>;\ntype DependentOperations = Record<string, number[]>;\n\nexport interface CacheExchangeOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionData;\n  storage?: StorageAdapter;\n}\n\nexport const cacheExchange = (opts?: CacheExchangeOpts): Exchange => ({\n  forward,\n  client,\n  dispatchDebug,\n}) => {\n  const store = new Store(opts);\n\n  let hydration: void | Promise<void>;\n  if (opts && opts.storage) {\n    hydration = opts.storage.readData().then(entries => {\n      hydrateData(store.data, opts!.storage!, entries);\n    });\n  }\n\n  const optimisticKeysToDependencies: OptimisticDependencies = new Map();\n  const mutationResultBuffer: OperationResult[] = [];\n  const ops: OperationMap = new Map();\n  const blockedDependencies: Dependencies = makeDict();\n  const requestedRefetch: Operations = new Set();\n  const deps: DependentOperations = makeDict();\n\n  const isBlockedByOptimisticUpdate = (dependencies: Dependencies): boolean => {\n    for (const dep in dependencies) if (blockedDependencies[dep]) return true;\n    return false;\n  };\n\n  const collectPendingOperations = (\n    pendingOperations: Operations,\n    dependencies: void | Dependencies\n  ) => {\n    if (dependencies) {\n      // Collect operations that will be updated due to cache changes\n      for (const dep in dependencies) {\n        const keys = deps[dep];\n        if (keys) {\n          deps[dep] = [];\n          for (let i = 0, l = keys.length; i < l; i++) {\n            pendingOperations.add(keys[i]);\n          }\n        }\n      }\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Operations\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    pendingOperations.forEach(key => {\n      if (key !== operation.key) {\n        const op = ops.get(key);\n        if (op) {\n          ops.delete(key);\n          let policy: RequestPolicy = 'cache-first';\n          if (requestedRefetch.has(key)) {\n            requestedRefetch.delete(key);\n            policy = 'cache-and-network';\n          }\n          client.reexecuteOperation(toRequestPolicy(op, policy));\n        }\n      }\n    });\n  };\n\n  // This registers queries with the data layer to ensure commutativity\n  const prepareForwardedOperation = (operation: Operation) => {\n    if (operation.kind === 'query') {\n      // Pre-reserve the position of the result layer\n      reserveLayer(store.data, operation.key);\n    } else if (operation.kind === 'teardown') {\n      // Delete reference to operation if any exists to release it\n      ops.delete(operation.key);\n      // Mark operation layer as done\n      noopDataState(store.data, operation.key);\n    } else if (\n      operation.kind === 'mutation' &&\n      operation.context.requestPolicy !== 'network-only'\n    ) {\n      // This executes an optimistic update for mutations and registers it if necessary\n      const { dependencies } = writeOptimistic(store, operation, operation.key);\n      if (!isDictEmpty(dependencies)) {\n        // Update blocked optimistic dependencies\n        for (const dep in dependencies) {\n          blockedDependencies[dep] = true;\n        }\n\n        // Store optimistic dependencies for update\n        optimisticKeysToDependencies.set(operation.key, dependencies);\n\n        // Update related queries\n        const pendingOperations: Operations = new Set();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n\n    return makeOperation(\n      operation.kind,\n      {\n        key: operation.key,\n        query: formatDocument(operation.query),\n        variables: operation.variables\n          ? filterVariables(\n              getMainOperation(operation.query),\n              operation.variables\n            )\n          : operation.variables,\n      },\n      operation.context\n    );\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Dependencies) => {\n    for (const dep in dependencies) {\n      (deps[dep] || (deps[dep] = [])).push(op.key);\n      ops.set(op.key, op);\n    }\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const res = query(store, operation);\n    const cacheOutcome: CacheOutcome = res.data\n      ? !res.partial\n        ? 'hit'\n        : 'partial'\n      : 'miss';\n\n    updateDependencies(operation, res.dependencies);\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data: res.data,\n      dependencies: res.dependencies,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (\n    result: OperationResult,\n    pendingOperations: Operations\n  ): OperationResult => {\n    const { operation, error, extensions } = result;\n    const { key } = operation;\n\n    if (operation.kind === 'mutation') {\n      // Collect previous dependencies that have been written for optimistic updates\n      const dependencies = optimisticKeysToDependencies.get(key);\n      collectPendingOperations(pendingOperations, dependencies);\n      optimisticKeysToDependencies.delete(key);\n    } else {\n      reserveLayer(store.data, operation.key);\n    }\n\n    let queryDependencies: void | Dependencies;\n    if (result.data) {\n      // Write the result to cache and collect all dependencies that need to be\n      // updated\n      const writeDependencies = write(\n        store,\n        operation,\n        result.data,\n        result.error,\n        key\n      ).dependencies;\n      collectPendingOperations(pendingOperations, writeDependencies);\n\n      const queryResult = query(\n        store,\n        operation,\n        result.data,\n        result.error,\n        key\n      );\n      result.data = queryResult.data;\n      if (operation.kind === 'query') {\n        // Collect the query's dependencies for future pending operation updates\n        queryDependencies = queryResult.dependencies;\n        collectPendingOperations(pendingOperations, queryDependencies);\n      }\n    } else {\n      noopDataState(store.data, operation.key);\n    }\n\n    // Update this operation's dependencies if it's a query\n    if (queryDependencies) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data: result.data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Buffer operations while waiting on hydration to finish\n    // If no hydration takes place we replace this stream with an empty one\n    const bufferedOps$ = hydration\n      ? pipe(\n          combine(\n            pipe(\n              sharedOps$,\n              scan((acc: Operation[], x) => (acc.push(x), acc), [])\n            ),\n            fromPromise(hydration)\n          ),\n          take(1),\n          mergeMap(zip => fromArray(zip[0]))\n        )\n      : (empty as Source<Operation>);\n\n    const inputOps$ = pipe(concat([bufferedOps$, sharedOps$]), share);\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cacheOps$ = pipe(\n      inputOps$,\n      filter(op => {\n        return (\n          op.kind === 'query' && op.context.requestPolicy !== 'network-only'\n        );\n      }),\n      map(operationResultFromCache),\n      share\n    );\n\n    const nonCacheOps$ = pipe(\n      inputOps$,\n      filter(op => {\n        return (\n          op.kind !== 'query' || op.context.requestPolicy === 'network-only'\n        );\n      })\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheMissOps$ = pipe(\n      cacheOps$,\n      filter(res => {\n        return (\n          res.outcome === 'miss' &&\n          res.operation.context.requestPolicy !== 'cache-only' &&\n          !isBlockedByOptimisticUpdate(res.dependencies)\n        );\n      }),\n      map(res => {\n        dispatchDebug({\n          type: 'cacheMiss',\n          message: 'The result could not be retrieved from the cache',\n          operation: res.operation,\n        });\n        return addCacheOutcome(res.operation, 'miss');\n      })\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cacheOps$,\n      filter(\n        res =>\n          res.outcome !== 'miss' ||\n          res.operation.context.requestPolicy === 'cache-only'\n      ),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome, dependencies } = res;\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            operation.context.requestPolicy === 'cache-and-network' ||\n            (operation.context.requestPolicy === 'cache-first' &&\n              outcome === 'partial')\n          ) {\n            result.stale = true;\n            if (!isBlockedByOptimisticUpdate(dependencies)) {\n              client.reexecuteOperation(\n                toRequestPolicy(operation, 'network-only')\n              );\n            } else if (\n              operation.context.requestPolicy === 'cache-and-network'\n            ) {\n              requestedRefetch.add(operation.key);\n            }\n          }\n\n          dispatchDebug({\n            type: 'cacheHit',\n            message: `A requested operation was found and returned from the cache.`,\n            operation: res.operation,\n            data: {\n              value: result,\n            },\n          });\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      merge([nonCacheOps$, cacheMissOps$]),\n      map(prepareForwardedOperation),\n      forward,\n      share\n    );\n\n    // Results that can immediately be resolved\n    const nonOptimisticResults$ = pipe(\n      result$,\n      filter(result => !optimisticKeysToDependencies.has(result.operation.key)),\n      map(result => {\n        const pendingOperations: Operations = new Set();\n        // Update the cache with the incoming API result\n        const cacheResult = updateCacheWithResult(result, pendingOperations);\n        // Execute all dependent queries\n        executePendingOperations(result.operation, pendingOperations);\n        return cacheResult;\n      })\n    );\n\n    // Prevent mutations that were previously optimistic from being flushed\n    // immediately and instead clear them out slowly\n    const optimisticMutationCompletion$ = pipe(\n      result$,\n      filter(result => optimisticKeysToDependencies.has(result.operation.key)),\n      mergeMap(\n        (result: OperationResult): Source<OperationResult> => {\n          const length = mutationResultBuffer.push(result);\n          if (length < optimisticKeysToDependencies.size) {\n            return empty;\n          }\n\n          for (let i = 0; i < mutationResultBuffer.length; i++) {\n            reserveLayer(store.data, mutationResultBuffer[i].operation.key);\n          }\n\n          for (const dep in blockedDependencies) {\n            delete blockedDependencies[dep];\n          }\n\n          const results: OperationResult[] = [];\n          const pendingOperations: Operations = new Set();\n\n          let bufferedResult: OperationResult | void;\n          while ((bufferedResult = mutationResultBuffer.shift()))\n            results.push(\n              updateCacheWithResult(bufferedResult, pendingOperations)\n            );\n\n          // Execute all dependent queries as a single batch\n          executePendingOperations(result.operation, pendingOperations);\n\n          return fromArray(results);\n        }\n      )\n    );\n\n    return merge([\n      nonOptimisticResults$,\n      optimisticMutationCompletion$,\n      cacheResult$,\n    ]);\n  };\n};\n","import { pipe, merge, makeSubject, share, filter } from 'wonka';\nimport { print, SelectionNode } from 'graphql';\n\nimport {\n  Operation,\n  Exchange,\n  ExchangeIO,\n  CombinedError,\n  createRequest,\n  makeOperation,\n} from '@urql/core';\n\nimport {\n  getMainOperation,\n  getFragments,\n  isInlineFragment,\n  isFieldNode,\n  shouldInclude,\n  getSelectionSet,\n  getName,\n} from './ast';\n\nimport {\n  SerializedRequest,\n  OptimisticMutationConfig,\n  Variables,\n} from './types';\n\nimport { makeDict } from './helpers/dict';\nimport { cacheExchange, CacheExchangeOpts } from './cacheExchange';\nimport { toRequestPolicy } from './helpers/operation';\n\n/** Determines whether a given query contains an optimistic mutation field */\nconst isOptimisticMutation = (\n  config: OptimisticMutationConfig,\n  operation: Operation\n) => {\n  const vars: Variables = operation.variables || makeDict();\n  const fragments = getFragments(operation.query);\n  const selections = [...getSelectionSet(getMainOperation(operation.query))];\n\n  let field: void | SelectionNode;\n  while ((field = selections.pop())) {\n    if (!shouldInclude(field, vars)) {\n      continue;\n    } else if (!isFieldNode(field)) {\n      const fragmentNode = !isInlineFragment(field)\n        ? fragments[getName(field)]\n        : field;\n      if (fragmentNode) selections.push(...getSelectionSet(fragmentNode));\n    } else if (config[getName(field)]) {\n      return true;\n    }\n  }\n\n  return false;\n};\n\nconst isOfflineError = (error: undefined | CombinedError) =>\n  error &&\n  error.networkError &&\n  !error.response &&\n  ((typeof navigator !== 'undefined' && navigator.onLine === false) ||\n    /request failed|failed to fetch|network\\s?error/i.test(\n      error.networkError.message\n    ));\n\nexport const offlineExchange = (opts: CacheExchangeOpts): Exchange => input => {\n  const { storage } = opts;\n\n  if (\n    storage &&\n    storage.onOnline &&\n    storage.readMetadata &&\n    storage.writeMetadata\n  ) {\n    const { forward: outerForward, client, dispatchDebug } = input;\n    const { source: reboundOps$, next } = makeSubject<Operation>();\n    const optimisticMutations = opts.optimistic || {};\n    const failedQueue: Operation[] = [];\n\n    const updateMetadata = () => {\n      const requests: SerializedRequest[] = [];\n      for (let i = 0; i < failedQueue.length; i++) {\n        const operation = failedQueue[i];\n        if (operation.kind === 'mutation') {\n          requests.push({\n            query: print(operation.query),\n            variables: operation.variables,\n          });\n        }\n      }\n      storage.writeMetadata!(requests);\n    };\n\n    let isFlushingQueue = false;\n    const flushQueue = () => {\n      if (!isFlushingQueue) {\n        isFlushingQueue = true;\n\n        for (let i = 0; i < failedQueue.length; i++) {\n          const operation = failedQueue[i];\n          if (operation.kind === 'mutation') {\n            next(makeOperation('teardown', operation));\n          }\n        }\n\n        for (let i = 0; i < failedQueue.length; i++)\n          client.reexecuteOperation(failedQueue[i]);\n\n        failedQueue.length = 0;\n        isFlushingQueue = false;\n        updateMetadata();\n      }\n    };\n\n    const forward: ExchangeIO = ops$ => {\n      return pipe(\n        outerForward(ops$),\n        filter(res => {\n          if (\n            res.operation.kind === 'mutation' &&\n            isOfflineError(res.error) &&\n            isOptimisticMutation(optimisticMutations, res.operation)\n          ) {\n            failedQueue.push(res.operation);\n            updateMetadata();\n            return false;\n          }\n\n          return true;\n        })\n      );\n    };\n\n    storage.onOnline(flushQueue);\n    storage.readMetadata().then(mutations => {\n      if (mutations) {\n        for (let i = 0; i < mutations.length; i++) {\n          failedQueue.push(\n            client.createRequestOperation(\n              'mutation',\n              createRequest(mutations[i].query, mutations[i].variables)\n            )\n          );\n        }\n\n        flushQueue();\n      }\n    });\n\n    const cacheResults$ = cacheExchange(opts)({\n      client,\n      dispatchDebug,\n      forward,\n    });\n\n    return ops$ => {\n      const sharedOps$ = share(ops$);\n      const opsAndRebound$ = merge([reboundOps$, sharedOps$]);\n\n      return pipe(\n        cacheResults$(opsAndRebound$),\n        filter(res => {\n          if (res.operation.kind === 'query' && isOfflineError(res.error)) {\n            next(toRequestPolicy(res.operation, 'cache-only'));\n            failedQueue.push(res.operation);\n            return false;\n          }\n\n          return true;\n        })\n      );\n    };\n  }\n\n  return cacheExchange(opts)(input);\n};\n"],"names":["a","map","arr","b","x","currentOperation","Set","index","fragmentNode","records","initDataState","ctx","writeField","node","Kind","input","g","vars","key","message","cache","console","doc","fragments","c","buildClientSchema","type","kind","fieldName","BUILTIN_FIELD_RE","getTypeCondition","typename","e","schema","invariant","interfaces","expectValidKeyingConfig","updates","const","warnAboutResolver","expectAbstractType","currentOptimisticKey","fieldKey","entityKey","dotIndex","batch","currentData","getNode","setNode","refCount","data","writeRecord","makeDict","layerKey","JSON","value","writeLink","h","has","commutativeKeys","optimistic","error","contextRef","keyOfField","getName","getFieldArguments","childIterator","undefined","getFragments","dataToWrite","makeContext","getFieldAlias","gc","fieldValue","ensureData","joinKeys","InMemoryData","getSelectionSet","rootNames","keyOfEntity","output","formatDocument","fragment","writeFragment","normalizeVariables","originalData","Array","newData","query","names","Object","store","select","result","NODE_ENV","getCurrentOperation","dataFieldValue","childResult","f","isDataOrKey","prevData","i","newLink","acc","cacheExchange","opts","dispatchDebug","res","blockedDependencies","dep","results","executePendingOperations","deps","process","skip","keymap","newCount","l","fieldInfos","ops","prepareForwardedOperation","isDictEmpty","this","optimisticKeysToDependencies","makeOperation","updateDependencies","reserveLayer","collectPendingOperations","extensions","mergeMap","take","concat","share","filter","operation","selections","config","failedQueue","isArray","updateMetadata","client","mutations","cacheResults$","storage"],"mappings":";;;;UAqESA;;;;;ACdgCC,SAAIC;;;;;WCxBxBC;;;;;;0BC7BOC;;;;SC4KnBC;;;;;2BAkBCC,qBAFmD;;;ACtCxB;EAAA;WAY1BC;;;;kDAUCC;;;;;;;gBCjHIC;;;;uCAoBdC;;;;;;AAyOIC,kBAAAA;;oBAMcC,EAAAA;;;;;;gBAQXT;;;;;;;;;;;;;;;;;;;;;;;;;;;SC1SaU;mBAGNC;;8BCrBdD;;;2BAOgBA;;;+BAehBE;WAEeF;;YAkBFG,UAXXC;;gCASFF;gDAEaC;;;;;;;;;oBAeqBD,EAAMG;;;;wBCtBtBJ;EAAAA;;;;;;YAsCCK,yDACdC,QACHC,MAAaF,+CACHA;;;;;;;kDTrDGG;SAMRC;;MAQHC;;;;;GCHOC;;yBA0BCC;;;;;2BALJC;MAUCxB;OAAAA;;;;;;;;;;oFC3DTyB;MAEIC;;;UAEG;;WA6Be1B;;MACA2B;MAClBJ,MAAkBK;YAAmCC;;;;cAQlDC;;KASQA;;;;;oBAmBfC,IAC8DC,cAAjCF;;;;;MAsBfG;;;;;;;;;0DA6BWC;gBACZT;;;;;;;;;;;;;;SAyDRU;;;;;;oBAcIL;;;;;4BACHM;;;;;;;;;;;;YClNCC;4EODFF,ENwDHG;;mBMjDiBC;SACF1C;6BAgBd2C;;MAGGC;mBAEW1B;SACVlB;;;;;;;;;eNmVL6C;eACAC,aAAAA,UAAAA;;;eAIaJ;;;;;;;;;;;;SAsCRK,OAAAA;;gBAUOJ;;;;;;;;;mBA0CdK;qBAEoBC;;;;4CAYRC;;;;;;;;;eAgENC;;;;wBAdgBC;2BAERN;gGAjCTI;;;;;eAoBHA;;;2BA8BwBG;;;;;;;;;;;;;;;;;;;;;;aAgDFnC;;;yCAIaoC,OAAWC,WAA1CC;;;;;;aC7iBN7C;6BAJgE8C,wCAqBtDC,+BACQ3B,iCALGe,YAQRa,gBARQD,gBAUV,WACKE;MAOZC;;;cAWGlD;uBAWPmD;eACAnD;qBACqBoB;aACrBpB;;;yBAG0BA;;;;;;;;;;QA6BPoD,WAAWC,OAAAA,sBAAeC,IAAwBhD;;;;;;WAoBpDiD;sBACAC;;;;;;mBC3BCC;;;;;;wBAakBC;;;;mBAexBC;;;kCAsBkB3D;eAUvB,eACoBgC;EADpB;;kBAeYoB;UACEQ;;;;UAIwBC;;;;;;;;;;;;;iBAuCzCC,YAAgCC;;gDAQlBC,MAOZC,gBANahE,IAEXiE,eAAAA,gBAFWjE,IAMbgE;;;;;;;iBAwEA7C;EAAAA,IA0BWY;;GAAAA,kDACFhC;;;;;;;;;;;;;;;;UKnSCsB;sCADNA;;;;;;;SAgDHiB,GACD4B;;;oBADC5B;WACD4B,uBAAA9C;;MAEAd;;;;;;WAmBmB;mBAC+BwB;SACnCyB,IAAAA,4BACNS;;MAWPjC;WAENT;;;;kBAekB6C;;;;;;gBAWdC;;;;YAOYC;;;;;4CAmBhB/B;2BAGoB+B,cAAeC,KAAnCC;;;;;;;;qBC7HAC;;;;;qCAwCEC;;YAEKA;;2BAODxE;;;;;mCAsBRwE;qBAEIC;cACIC;;;;;;MAgBU5E;;mBAiBa6E;MACzBC,IAAQC;MAEV;;;;MAacC;;;;;;;kDAuClBC,MAEAC,gEAMkBlF;UAkBdO,oCAHa4E;;;;;;wBA0BWjF;WAcR;;;;;;;gBAeU,WAA1BkF;oCAMmB7C;;;;;;;sBA2FnBA;;2BACS8C;;;;;;;;;;;;;YAkCHC;WAgBJ/C;;;;;;;yDAOkBgD;MAIbC;eACKC,KAAYjD,YACnBhD;;qBAiBTQ;;;;2BAyBIA;;UAKU0F,EAARC;;;;;;;;;;;;;;UCvQqCC;;;;;;;;;;;IAhNhCC,yBAAiBC;;;;;2CA2PtBC,oBAEEvF;6BAGqBwF;;wBAXrBA;;;;;;;;;;;;;;;;;;sBAqGehB;;WAINiB,EAAoBC;;;oBAQ3BC;;OAKFC;;;;;;mBAnWIN;;;;;;;mBAyBOO;;;;;;;;;;OAWbD,oDT/BN1G;;;yBAGqB4G;;;6BAqEjBvG,kCAAAA;IAAAA;QACA8D;;4CAGarE,EAAb+C;QAlCEA;;;;;;;;uCA2BkCA,iBACtCA,MAAa1B;;;;;QAkFX+B,MA0BF2D,GAgDS5B;QAjFInB,iBACbgD,QAAgC/D;WAM9BG,KAAwBd;wBAS5BxC;gBAOIiH;;iBAqBYxE;;;;8BAMT7B;;eAaoB8B,MAArByE;iDAK8B;;;;iBAazB9B;sBACO+B,kBAAqBA,IAAGhB;;;;;;;;;qCA6B1CiB;;eAUqBrH;oDAE2C0C;;;;;;;;sBSpOxD4E;;;;;OAaFC;;;;WAMFD;;;4BASKE,IAA2BC;;;;AAO9BC;;;;;oCASGC;;;;;gCA8B4BjB,QAC9BA;yBAKLkB;;;;UAkBuB7H;;;AAIrB2H,gBAAAA;+EAEAG;;;;AA2BEC,gBAAAA;;uBAGYpC;;;8CAQmBqC;;;;gCAkB7BC,SADAC;sBAKiBC,KAAAA,eAAoCC;;;oBAiCzDnI;0CAcAoI;;;;;oEChSEpH,IAAkBqH;MAELhE,mDAAIO;mBAGP0D;;;SAIN/H;;aAIGgI;;;;;;;;;;;;;;;;;;mBA4DPC;YACkBC;SAClBC;;;;;;;;+EA4BIC,uBAEgBC;;;;;;;0BAoBpBC;;;;;;2BA1BJC;8BAesBvC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"}